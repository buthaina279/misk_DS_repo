---
title: "ex3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
library(caret)
```


```{r}
ames <- AmesHousing::make_ames()
```

#Using the Ames dataset and the same approach shown in the last section:

##1 
Rather than use a 70% stratified training split, try an 80% unstratified training split.

```{r}
# 1. stratified sampling with the rsample package
set.seed(123)
split  <- initial_split(ames, prop = 0.8)
ames_train  <- training(split)
ames_test   <- testing(split)

# 2. Feature engineering
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

# 3. create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# 4. create a hyperparameter grid search
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))
```

How does your cross-validated results compare?

```{r}
# 5. execute grid search with knn model
#    use RMSE as preferred metric
knn_fit <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
```

```{r}
# 6. evaluate results
# print model results
knn_fit

```


```{r}
# plot cross validation results
ggplot(knn_fit$results, aes(k, RMSE)) + 
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::dollar)
```


##2
Rather than numerically encode the quality and condition features (i.e. step_integer(matches("Qual|Cond|QC|Qu"))), 

one-hot encode these features. 
What is the difference in the number of features your training set? 
```{r}
# 2. Feature engineering
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_dummy(matches("Qual|Cond|QC|Qu"), one_hot = TRUE) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```


Apply the same cross-validated KNN model to this new feature set. 
How does the performance change?
How does the training time change?

```{r}
knn_fit_2 <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
```


##3

Identify three new step_xxx functions that recipes provides:
- step_YeoJohnson()
- step_other()
- step_pca()

Why would these feature engineering steps be applicable to the Ames data?
- step_YeoJohnson():
to minimize the skewness of numeric features

- step_other():
Because sometimes features will contain levels that have very few observations

- step_pca():
to filter out non-informative features without manually removing them


****Apply these feature engineering steps along with the same cross-validated KNN model. 

*****How do your results change?

```{r}


blueprint_3 <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
step_YeoJohnson(all_numeric()) %>% 
step_other(Neighborhood, threshold = 0.01, 
             other = "other") %>%
  step_other(Screen_Porch, threshold = 0.1, 
             other = ">0") %>% 
 step_pca(all_numeric(), threshold = .95)
```


```{r}
knn_fit_3 <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
```


##4 
Using the Attrition data set,
```{r}
library(modeldata)
data(attrition)
```

assess the characterstics of the target and features.

Which target/feature engineering steps should be applied?
```{r}

```

Create a feature engineering blueprint and apply a KNN grid search. 

What is the performance of your model?

```{r}

```


